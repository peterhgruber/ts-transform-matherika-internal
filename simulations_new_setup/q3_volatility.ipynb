{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 â€“ Sensitivity to Volatility Regimes\n",
    "\n",
    "This notebook investigates how well models capture different volatility regimes in financial time series. We analyze KL divergence between model forecasts and true distributions, conditioning on the known volatility of each data-generating process (DGP). This helps identify which models generalize better under varying levels of uncertainty.\n",
    "\n",
    "### Objective\n",
    "- Assess how forecast quality changes across DGPs with increasing volatility.\n",
    "- Identify which models are robust in high- and low-volatility environments.\n",
    "- Separate evaluation for price and return forecasts.\n",
    "\n",
    "### DGP Volatility Spectrum\n",
    "\n",
    "The DGPs span a wide range of volatility regimes, from nearly deterministic to highly stochastic. Volatility levels are empirically estimated by computing the **standard deviation of returns on the last forecast day (Day 22)** across all simulated paths, then **annualized** using a `âˆš252` scaling factor.\n",
    "\n",
    "This results in the following volatility ordering:\n",
    "\n",
    "- **Very Low Volatility**: `gbm_low_vol` (â‰ˆ8% annualized)\n",
    "- **Low Volatility**: `mixture_normal`\n",
    "- **Medium Volatility**: `seasonal`\n",
    "- **High Volatility**: `t_garch` (heavy tails and time-varying volatility)\n",
    "- **Very High Volatility**: `gbm_high_vol` (â‰ˆ80% annualized)\n",
    "\n",
    "### Key Outputs\n",
    "- ðŸ“„ KL divergence tables:\n",
    "  - Sorted by DGP volatility\n",
    "  - Shown separately for prices and returns\n",
    "- ðŸ“Š Bar plots:\n",
    "  - KL divergence vs. DGP volatility for each model\n",
    "\n",
    "### Notes\n",
    "- Context length is held fixed during each comparison.\n",
    "- KL is computed at three forecast horizons: Day 2, Day 12, and Day 22.\n",
    "- Only models in `selected_model_names` are evaluated.\n",
    "\n",
    "This setup allows us to systematically evaluate how model performance varies with increasing market volatilityâ€”an essential consideration in risk-sensitive financial modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.evaluation import (\n",
    "    compute_kl_divergence,\n",
    "    format_pivot_table,\n",
    "    dataframe_to_latex\n",
    ")\n",
    "\n",
    "from utils.plotting import plot_kl_bar_by_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models List\n",
    "\n",
    "Models can be added or removed from the followng list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Models for Analysis\n",
    "selected_model_names = [\n",
    "    \"chronos_model_tiny\",\n",
    "    \"chronos_model_mini\",\n",
    "    \"chronos_model_base\",\n",
    "    \"lag_llama_model\",\n",
    "    \"moirai_model_small\",\n",
    "    \"moirai_model_base\",\n",
    "    \"toto_model\",\n",
    "    \"tirex_model\",\n",
    "    \"timesfm_model_small\",\n",
    "    \"timesfm_model_large\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Setup\n",
    "results_dir = Path(\"results_q3_volatility\")\n",
    "tables_dir = results_dir / \"tables\"\n",
    "plots_bar_dir = results_dir / \"plots_bar\"\n",
    "plots_bar_dir.mkdir(parents=True, exist_ok=True)\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "forecast_dir = Path(\"forecasts\")\n",
    "run_dir = Path(\"runfiles\")\n",
    "datasets_dir = Path(\"datasets\")\n",
    "\n",
    "selected_days = [0, 10, 20]\n",
    "context_lengths = [22, 66, 252]\n",
    "dgp_types_kl = [\"gbm_low_vol\", \"mixture_normal\", \"seasonal\", \"t_garch\", \"gbm_high_vol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Forecasts\n",
    "\n",
    "We load the forecasts and retrieve the specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forecasts\n",
    "forecast_files = sorted(forecast_dir.glob(\"forecast_*.pkl\"))\n",
    "results = []\n",
    "\n",
    "for forecast_file in forecast_files:\n",
    "    run_name = forecast_file.stem\n",
    "    run_file = run_dir / f\"{run_name}.txt\"\n",
    "    if not run_file.exists():\n",
    "        continue\n",
    "\n",
    "    run_config = {}\n",
    "    with open(run_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"=\" in line:\n",
    "                key, value = [x.strip() for x in line.strip().split(\"=\", 1)]\n",
    "                try:\n",
    "                    run_config[key] = eval(value)\n",
    "                except:\n",
    "                    run_config[key] = value.strip(\"\\\"'\").strip(\"'\")\n",
    "\n",
    "    try:\n",
    "        with open(forecast_file, \"rb\") as f:\n",
    "            forecast_result = pickle.load(f)\n",
    "            low, median, high, samples, base_price = forecast_result\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    results.append({\n",
    "        \"run_name\": run_name,\n",
    "        \"model_name\": run_config[\"model_name\"],\n",
    "        \"dgp_type\": run_config[\"dataset_name\"],\n",
    "        \"target_type\": run_config[\"target_type\"],\n",
    "        \"context_length\": run_config[\"context_length\"],\n",
    "        \"samples\": samples,\n",
    "        \"low\": low,\n",
    "        \"median\": median,\n",
    "        \"high\": high,\n",
    "        \"base_price\": base_price\n",
    "    })\n",
    "\n",
    "# Filter Results by Selected Models\n",
    "results = [r for r in results if r[\"model_name\"] in selected_model_names]\n",
    "\n",
    "price_results = [r for r in results if r[\"target_type\"] == \"prices\"]\n",
    "return_results = [r for r in results if r[\"target_type\"] == \"returns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions\n",
    "\n",
    "We define 3 new special functions to compute volatility, save tables and compute the KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Annualized Volatility of True DGP (last day, across paths)\n",
    "def compute_dgp_volatilities(dgp_types, datasets_dir, is_price=True, last_day_index=20):\n",
    "    dgp_vols = {}\n",
    "    for dgp in dgp_types:\n",
    "        file_path = datasets_dir / (f\"{dgp}_paths.npy\" if is_price else f\"{dgp}_returns_paths.npy\")\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "\n",
    "        data = np.load(file_path)\n",
    "        returns = data[:, 1:] / data[:, :-1] - 1 if is_price else data\n",
    "        last_day_vol = np.std(returns[:, last_day_index])\n",
    "        annualized_vol = last_day_vol * np.sqrt(252)\n",
    "        dgp_vols[dgp] = round(annualized_vol, 4)\n",
    "\n",
    "    return dgp_vols\n",
    "\n",
    "dgp_volatility_map = compute_dgp_volatilities(dgp_types_kl, datasets_dir, is_price=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute KL Divergence and Attach DGP Volatility\n",
    "def compute_kl_and_vol(results_subset, dgp_vol_map):\n",
    "    df_rows = []\n",
    "\n",
    "    for item in results_subset:\n",
    "        if item[\"dgp_type\"] not in dgp_types_kl:\n",
    "            continue\n",
    "\n",
    "        is_price = item[\"target_type\"] == \"prices\"\n",
    "        model_returns = item[\"samples\"]\n",
    "        if is_price:\n",
    "            model_returns = model_returns[:, 1:] / model_returns[:, :-1] - 1\n",
    "\n",
    "        dgp_path = datasets_dir / f\"{item['dgp_type']}_returns_paths.npy\"\n",
    "        if not dgp_path.exists():\n",
    "            continue\n",
    "\n",
    "        dgp_returns = np.load(dgp_path)\n",
    "\n",
    "        for day_index in selected_days:\n",
    "            try:\n",
    "                p = dgp_returns[:, day_index]\n",
    "                q = model_returns[:, day_index]\n",
    "                kl = compute_kl_divergence(p, q)\n",
    "                df_rows.append({\n",
    "                    \"context_length\": item[\"context_length\"],\n",
    "                    \"dgp_type\": item[\"dgp_type\"],\n",
    "                    \"model_name\": item[\"model_name\"],\n",
    "                    \"day\": f\"Day {day_index + 2}\",\n",
    "                    \"kl_divergence\": kl,\n",
    "                    \"volatility\": dgp_vol_map[item[\"dgp_type\"]]\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(df_rows).round(4)\n",
    "\n",
    "df_kl_prices = compute_kl_and_vol(price_results, dgp_volatility_map)\n",
    "df_kl_returns = compute_kl_and_vol(return_results, dgp_volatility_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate KL Tables Sorted by Volatility\n",
    "def save_kl_table_sorted_by_vol(df_kl, label):\n",
    "    for context in context_lengths:\n",
    "        df_filtered = df_kl[df_kl[\"context_length\"] == context]\n",
    "        pivot = df_filtered.pivot_table(\n",
    "            index=[\"context_length\", \"volatility\", \"dgp_type\", \"model_name\"],\n",
    "            columns=\"day\",\n",
    "            values=\"kl_divergence\"\n",
    "        ).sort_index(level=\"volatility\")\n",
    "\n",
    "        formatted = format_pivot_table(pivot, selected_days)\n",
    "        filename = f\"q3_kl_by_vol_{label}_context{context}.tex\"\n",
    "        dataframe_to_latex(formatted, tables_dir / filename, preserve_index_order=True)\n",
    "\n",
    "save_kl_table_sorted_by_vol(df_kl_prices, \"prices\")\n",
    "save_kl_table_sorted_by_vol(df_kl_returns, \"returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Only the specific figure is here plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate KL Bar Plot with Volatility on X-axis\n",
    "for context_val in context_lengths:\n",
    "    plot_kl_bar_by_vol(df_kl_prices, plots_bar_dir, \"prices\", context_val)\n",
    "    plot_kl_bar_by_vol(df_kl_returns, plots_bar_dir, \"returns\", context_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: How Does Volatility Influence Forecasting Performance?\n",
    "\n",
    "We assess model robustness across DGPs with varying volatility, using KL divergence at context length 22. Lower KL indicates better alignment between predicted and ground-truth distributions. Both price and return forecasts are analyzed.\n",
    "\n",
    "**Price Forecasts**\n",
    "\n",
    "- High volatility (gbm_high_vol) is well handled by most models. Chronos, Toto, Tirex, and TimesFM large maintain low KL values even at Day 22. Lag-Llama also performs well.\n",
    "\n",
    "- Low volatility (gbm_low_vol) creates instability. Chronos base and mini spike at longer horizons. TimesFM small and Moirai small degrade sharply. Tirex and Chronos tiny remain more stable.\n",
    "\n",
    "- Mixture normal shows moderate divergence. Chronos and Lag-Llama handle it well. Toto performs inconsistently, with late-horizon spikes. Moirai and TimesFM show steady error growth.\n",
    "\n",
    "- T-garch is challenging. Chronos and TimesFM small struggle. Tirex and Toto manage better after Day 12. Lag-Llama and Moirai base show strong adaptation.\n",
    "\n",
    "- Seasonal is captured well by Chronos and Lag-Llama. Moirai and TimesFM small degrade on later horizons. Toto performs consistently.\n",
    "\n",
    "**Return Forecasts**\n",
    "\n",
    "- High-volatility returns are difficult for Chronos (KL > 13 across DGPs). Lag-Llama and Toto remain robust with near-zero KL. Moirai and TimesFM vary with size.\n",
    "\n",
    "- Low volatility returns again highlight poor performance from Chronos (KL ~15+). Lag-Llama and Toto are highly accurate. Tirex, Moirai small, and TimesFM large do well.\n",
    "\n",
    "- Mixture normal return divergence is wide. Chronos is high. Toto and Lag-Llama remain strong. TimesFM and Moirai perform moderately well.\n",
    "\n",
    "- T-garch is toughest overall. Lag-Llama, Moirai, and Toto show good late-horizon accuracy. Chronos and Tirex exhibit large, persistent error.\n",
    "\n",
    "- Seasonal returns are best captured by Lag-Llama and Toto (KL â‰ˆ 0). Chronos performs poorly again. Moirai and TimesFM are stable, but not top-tier.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Volatility impacts model performance in nuanced ways:\n",
    "\n",
    "- High volatility does not always increase KL; some models generalize better in noisy regimes.\n",
    "- Chronos struggles across the board in return forecasting.\n",
    "- Lag-Llama and Toto are the most consistent performers across volatility levels and forecast types.\n",
    "- Moirai and TimesFM show size-dependent variability.\n",
    "- Tirex excels in price forecasting but is less stable in return space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
