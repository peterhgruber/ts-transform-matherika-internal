{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 â€“ Effect of Context Length on Forecast Distributions\n",
    "\n",
    "This notebook analyzes how increasing the context window affects forecast quality across different models and DGPs.\n",
    "\n",
    "### Objective\n",
    "- Evaluate distributional changes as context length varies.\n",
    "- Summarize return statistics and KL divergence per context.\n",
    "\n",
    "### Key Outputs\n",
    "- ðŸ“„ Tables:\n",
    "  - Return mean/std across selected forecast days\n",
    "  - Percentile tables (1%â€“99%)\n",
    "  - KL divergence per context and DGP\n",
    "\n",
    "- ðŸ“Š Plots:\n",
    "  - KL divergence vs. context length\n",
    "\n",
    "### Notes\n",
    "- Focused on all models listed in `selected_model_names`\n",
    "- KL computed between model forecast and DGP distribution at selected days\n",
    "- Forecast days: `Day 2`, `Day 12`, `Day 22`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.plotting import plot_kl_vs_context\n",
    "\n",
    "from utils.evaluation import (\n",
    "    compute_kl_divergence,\n",
    "    format_pivot_table,\n",
    "    dataframe_to_latex\n",
    ")\n",
    "\n",
    "# Needed to avoid issues with numpy for TimesFM 2.5\n",
    "import sys, numpy.core.numeric as numeric\n",
    "sys.modules['numpy._core.numeric'] = numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models List\n",
    "\n",
    "Models can be added or removed from the followng list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Models for Analysis\n",
    "selected_model_names = [\n",
    "    \"chronos_model_tiny\",\n",
    "    \"chronos_model_mini\",\n",
    "    \"chronos_model_base\",\n",
    "    \"lag_llama_model\",\n",
    "    \"moirai_model_small\",\n",
    "    \"moirai_model_base\",\n",
    "    \"moirai_model_small_2_0\",   # NEW\n",
    "    \"moirai_model_small_1_1\",   # NEW\n",
    "    \"moirai_model_base_1_1\",    # NEW\n",
    "    \"toto_model\",\n",
    "    \"tirex_model\",\n",
    "    \"timesfm_model_small\",\n",
    "    \"timesfm_model_large\",\n",
    "    \"timesfm_model_2_5\"        # NEW\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Setup\n",
    "results_dir = Path(\"results_q1_context\")\n",
    "tables_dir = results_dir / \"tables\"\n",
    "plots_context_dir = results_dir / \"plots_context\"\n",
    "plots_context_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for folder in [tables_dir, plots_context_dir]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "forecast_dir = Path(\"forecasts\")\n",
    "run_dir = Path(\"runfiles\")\n",
    "datasets_dir = Path(\"datasets\")\n",
    "\n",
    "selected_days = [0, 10, 20]\n",
    "ordered_days = [f\"Day {d+2}\" for d in selected_days]\n",
    "ordered_percentiles = [\"p1%\", \"p5%\", \"p25%\", \"p50%\", \"p75%\", \"p95%\", \"p99%\"]\n",
    "percentile_values = [int(p.replace(\"p\", \"\").replace(\"%\", \"\")) for p in ordered_percentiles]\n",
    "\n",
    "dgp_types_kl = [\"gbm_low_vol\", \"gbm_high_vol\", \"garch\", \"t_garch\", \"mixture_normal\", \"seasonal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Forecasts\n",
    "\n",
    "We load the forecasts and retrieve the specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forecasts\n",
    "forecast_files = sorted(forecast_dir.glob(\"forecast_*.pkl\"))\n",
    "results = []\n",
    "\n",
    "for forecast_file in forecast_files:\n",
    "    run_name = forecast_file.stem\n",
    "    run_file = run_dir / f\"{run_name}.txt\"\n",
    "    if not run_file.exists():\n",
    "        continue\n",
    "\n",
    "    run_config = {}\n",
    "    with open(run_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"=\" in line:\n",
    "                key, value = [x.strip() for x in line.strip().split(\"=\", 1)]\n",
    "                try:\n",
    "                    run_config[key] = eval(value)\n",
    "                except:\n",
    "                    run_config[key] = value.strip(\"\\\"'\").strip(\"'\")\n",
    "\n",
    "    try:\n",
    "        with open(forecast_file, \"rb\") as f:\n",
    "            forecast_result = pickle.load(f)\n",
    "            low, median, high, samples, base_price = forecast_result\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    results.append({\n",
    "        \"run_name\": run_name,\n",
    "        \"model_name\": run_config[\"model_name\"],\n",
    "        \"dgp_type\": run_config[\"dataset_name\"],\n",
    "        \"target_type\": run_config[\"target_type\"],\n",
    "        \"context_length\": run_config[\"context_length\"],\n",
    "        \"samples\": samples,\n",
    "        \"low\": low,\n",
    "        \"median\": median,\n",
    "        \"high\": high,\n",
    "        \"base_price\": base_price\n",
    "    })\n",
    "\n",
    "# Filter Results by Selected Models\n",
    "results = [r for r in results if r[\"model_name\"] in selected_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by Target Type\n",
    "price_results = [r for r in results if r[\"target_type\"] == \"prices\"]\n",
    "return_results = [r for r in results if r[\"target_type\"] == \"returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique model names found in runfiles:\n",
      "'chronos_model_base'\n",
      "'chronos_model_mini'\n",
      "'chronos_model_tiny'\n",
      "'lag_llama_model'\n",
      "'moirai_model_base'\n",
      "'moirai_model_base_1_1'\n",
      "'moirai_model_small'\n",
      "'moirai_model_small_1_1'\n",
      "'moirai_model_small_2_0'\n",
      "'timesfm_model_2_5'\n",
      "'timesfm_model_large'\n",
      "'timesfm_model_small'\n",
      "'tirex_model'\n",
      "'toto_model'\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique model names found in runfiles:\")\n",
    "for name in sorted(set(r[\"model_name\"] for r in results)):\n",
    "    print(f\"'{name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions\n",
    "\n",
    "We define 2 new special functions to save tables and compute the KL divergence compatible with this notebook setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Summary and Percentiles\n",
    "def process_and_save_tables(results_subset, label):\n",
    "    summary_rows = []\n",
    "    percentile_rows = []\n",
    "\n",
    "    for item in results_subset:\n",
    "        is_price = item[\"target_type\"] == \"prices\"\n",
    "        returns = item[\"samples\"]\n",
    "        if is_price:\n",
    "            returns = returns[:, 1:] / returns[:, :-1] - 1\n",
    "\n",
    "        for day in selected_days:\n",
    "            daily = returns[:, day]\n",
    "            summary_rows.append({\n",
    "                \"context_length\": item[\"context_length\"],\n",
    "                \"dgp_type\": item[\"dgp_type\"],\n",
    "                \"model_name\": item[\"model_name\"],\n",
    "                \"day\": f\"Day {day+2}\",\n",
    "                \"mean_return (%)\": np.mean(daily) * 100,\n",
    "                \"std_return (%)\": np.std(daily) * 100\n",
    "            })\n",
    "\n",
    "            for p, v in zip(ordered_percentiles, np.percentile(daily, percentile_values)):\n",
    "                percentile_rows.append({\n",
    "                    \"context_length\": item[\"context_length\"],\n",
    "                    \"dgp_type\": item[\"dgp_type\"],\n",
    "                    \"model_name\": item[\"model_name\"],\n",
    "                    \"day\": f\"Day {day+2}\",\n",
    "                    \"percentile\": p,\n",
    "                    \"return (%)\": v * 100\n",
    "                })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_rows).round(2)\n",
    "    df_percent = pd.DataFrame(percentile_rows).round(2)\n",
    "\n",
    "    pivot_summary = df_summary.pivot_table(\n",
    "        index=[\"context_length\", \"dgp_type\", \"model_name\"],\n",
    "        columns=\"day\",\n",
    "        values=[\"mean_return (%)\", \"std_return (%)\"]\n",
    "    )\n",
    "    dataframe_to_latex(format_pivot_table(pivot_summary, selected_days), tables_dir / f\"forecast_table_{label}.tex\")\n",
    "\n",
    "    pivot_percent = df_percent.pivot_table(\n",
    "        index=[\"context_length\", \"dgp_type\", \"model_name\"],\n",
    "        columns=[\"day\", \"percentile\"],\n",
    "        values=\"return (%)\"\n",
    "    )\n",
    "    ordered_columns = pd.MultiIndex.from_product([ordered_days, ordered_percentiles])\n",
    "    pivot_percent = pivot_percent.reindex(columns=ordered_columns)\n",
    "    dataframe_to_latex(format_pivot_table(pivot_percent, selected_days), tables_dir / f\"percentiles_table_{label}.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence Table\n",
    "def compute_and_save_kl(results_subset, label):\n",
    "    rows = []\n",
    "    for item in results_subset:\n",
    "        if item[\"dgp_type\"] not in dgp_types_kl:\n",
    "            continue\n",
    "\n",
    "        is_price = item[\"target_type\"] == \"prices\"\n",
    "        model_returns = item[\"samples\"]\n",
    "        if is_price:\n",
    "            model_returns = model_returns[:, 1:] / model_returns[:, :-1] - 1\n",
    "\n",
    "        dgp_path = datasets_dir / f\"{item['dgp_type']}_returns_paths.npy\"\n",
    "\n",
    "        if not dgp_path.exists():\n",
    "            continue\n",
    "\n",
    "        dgp_returns = np.load(dgp_path)\n",
    "\n",
    "        for day_index in selected_days:\n",
    "            try:\n",
    "                p = dgp_returns[:, day_index]\n",
    "                q = model_returns[:, day_index]\n",
    "                kl = compute_kl_divergence(p, q)\n",
    "                rows.append({\n",
    "                    \"context_length\": item[\"context_length\"],\n",
    "                    \"dgp_type\": item[\"dgp_type\"],\n",
    "                    \"model_name\": item[\"model_name\"],\n",
    "                    \"day\": f\"Day {day_index+2}\",\n",
    "                    \"kl_divergence\": kl\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    df_kl = pd.DataFrame(rows).round(4)\n",
    "    filename = f\"kl_divergence_table_{label}.tex\"\n",
    "    pivot_kl = df_kl.pivot_table(\n",
    "        index=[\"context_length\", \"dgp_type\", \"model_name\"],\n",
    "        columns=\"day\",\n",
    "        values=\"kl_divergence\"\n",
    "    )\n",
    "    dataframe_to_latex(format_pivot_table(pivot_kl, selected_days), tables_dir / filename)\n",
    "    return df_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Table Generation\n",
    "process_and_save_tables(price_results, \"prices\")\n",
    "process_and_save_tables(return_results, \"returns\")\n",
    "\n",
    "df_kl_prices = compute_and_save_kl(price_results, \"prices\")\n",
    "df_kl_returns = compute_and_save_kl(return_results, \"returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Only the specific figure is here plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Context vs KL Divergence\n",
    "plot_kl_vs_context(df_kl_prices, plots_context_dir, \"prices\")\n",
    "plot_kl_vs_context(df_kl_returns, plots_context_dir, \"returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Context Length Effect on Forecast Performance\n",
    "\n",
    "We assess how increasing context length (22 â†’ 66 â†’ 252) affects forecast quality, using KL divergence as the primary metric. Lower KL indicates better alignment between model forecasts and the true data-generating process (DGP). The analysis emphasizes well-performing models to understand which architectures benefit most from longer histories.\n",
    "\n",
    "**Returns Forecasts**\n",
    "\n",
    "Several models clearly benefit from extended context:\n",
    "\n",
    "- Moirai (base and small) improves steadily across most processes. For example, on the high-volatility GBM, KL divergence drops sharply with longer context. Similar gains are observed for the GARCH and mixture-normal processes, suggesting Moirai is able to extract more accurate return distributions from extended sequences.\n",
    "\n",
    "- Toto shows consistent or improved KL divergence across all context lengths. By context 252, it delivers among the lowest KL values across nearly all DGPs, particularly for GBM and GARCH. This points to strong generalization and robustness in modeling return behavior.\n",
    "\n",
    "- Tirex maintains stable and low KL values across contexts. While the improvement from longer context is more modest than Moirai, the model performs reliably well even at short history lengths.\n",
    "\n",
    "- Lag-Llama performs exceptionally at short context, with KL values already near zero. Its performance remains saturated across longer contexts, meaning it achieves high-quality return forecasts even without extended history.\n",
    "\n",
    "- Chronos models (base, mini, tiny) show mixed patterns. While KL divergence decreases slightly with longer contextâ€”especially for more complex DGPs like GARCH and seasonalâ€”it remains substantially higher than top performers. This suggests Chronos may struggle to extract full benefit from longer return histories.\n",
    "\n",
    "**Price Forecasts**\n",
    "\n",
    "The effect of context length on price-level forecasts is less pronounced and sometimes inconsistent:\n",
    "\n",
    "- Moirai models tend to improve or remain stable across contexts. On the GARCH process, for example, KL divergence drops significantly with more context in the base version, indicating enhanced alignment to the true price dynamics.\n",
    "\n",
    "- Lag-Llama shows signs of instability with longer price sequences. While excellent in the return space, it exhibits rising KL divergence for several processesâ€”such as high-volatility GBMâ€”where KL can spike beyond 8. This could reflect overfitting or misalignment when processing long price series.\n",
    "\n",
    "- Toto remains generally robust, with low KL divergence across all contexts. It does, however, show occasional spikes on isolated DGPs, pointing to some sensitivity in its price-level modeling.\n",
    "\n",
    "- Chronos and TimesFM exhibit moderate improvement or plateauing. Chronos, for instance, shows decreasing KL for low-volatility GBM with more context, but overall KL levels remain higher than those of Moirai or Toto, particularly at longer horizons.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Longer context windows benefit most models in the return space, where capturing the underlying distribution is more sensitive to history. Moirai, Toto, and Tirex all demonstrate improved KL scores with context and consistently strong performance overall. Lag-Llama achieves high-quality return modeling even at short contexts, but its price forecasts may degrade with longer sequences. Chronos and TimesFM show limited gains, suggesting potential architectural constraints in extracting information from long temporal inputs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
